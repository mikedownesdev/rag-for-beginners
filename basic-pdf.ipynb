{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cb59e8-3e5d-4177-ad33-3a2ec96ed474",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "This python notebook aims to take the concepts of a RAG System and put them into practice with some working code. There's an emphasis here to write the imperative steps, and not rely on packages or libraries that abstract away the inner workings, so that we can better understand the plumbing of such a system. The overall process looks like this:\n",
    "\n",
    "- Read your local data files\n",
    "- Chunk the data for efficiency\n",
    "- Embed the data as vectors, making it easier for models to complete similarity search\n",
    "- Store the vectors\n",
    "- Retrieve relevant chunks of data for a user query\n",
    "- Pass the query and relevant chunks to an LLM for response generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ad488-9491-49cf-bb2e-edca5ed22e42",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8a970-08ff-4844-b67c-f0f5a059e626",
   "metadata": {},
   "source": [
    "In this simple example we'll be reading a single PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73adedc8-4fff-4951-acb3-4fe38da84f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to add this to suppress a warning during the embedding process below. Not part of the tutorial really...\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a19c0daa-4fa3-49e7-829d-dc748f4d38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba770862-2bff-4ac1-8302-a520c0d5673e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drylab Newsfor in vestors & friends · Ma y 2017\n",
      "Welcome to our first newsletter of 2017! It's\n",
      "been a while since the last one, and a lot has\n",
      "happened. W e promise to k eep them coming\n",
      "every two months\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "# Using a raw string, absolute path\n",
    "FILE_PATH = r\"/Users/michaeldownes/Library/Mobile Documents/iCloud~md~obsidian/Documents/Cloud Vault/Attachments/EXAMPLE-PDF.pdf\"\n",
    "\n",
    "reader = PdfReader(FILE_PATH)\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "pdf_text = \"\"\n",
    "for page_num in range(number_of_pages):\n",
    "    page = reader.pages[page_num]\n",
    "    pdf_text += page.extract_text()\n",
    "\n",
    "print(pdf_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a060c-c28e-43be-9262-55a72a934492",
   "metadata": {},
   "source": [
    "## Chunk time (Simple)\n",
    "\n",
    "We'll use a simple fixed-length chunk technique here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91139128-0565-4a4c-889f-b81e9a7d189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 14\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "text_chunks = text_splitter.split_text(pdf_text)\n",
    "print(f\"Total chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a5ccad-27c7-4545-a13e-c576c3c60a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Drylab Newsfor in vestors & friends · Ma y 2017\\nWelcome to our first newsletter of 2017! It's\\nbeen a while since the last one, and a lot has\\nhappened. W e promise to k eep them coming\\nevery two months hereafter , and permit\\nourselv es to mak e this one r ather long. The\\nbig news is the beginnings of our launch in\\nthe American mark et, but there are also\\ninteresting updates on sales, de velopment,\\nmentors and ( of course ) the in vestment\\nround that closed in January .\",\n",
       " 'round that closed in January .\\nNew c apital: The in vestment round was\\nsuccessful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation\\nNorwa y. Including the de velopment\\nagreement with Filmlance International, the\\ntotal new capital is 5 MNOK, partly tied to\\nthe successful completion of milestones. All\\nformalities associated with this process are\\nnow finalized.\\nNew o wners: We would especially lik e to\\nwarmly welcome our new owners to the']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dece4e2-8714-4744-bb28-dcbea470f9e3",
   "metadata": {},
   "source": [
    "## Embedd the chunks\n",
    "\n",
    "Let's take our chunks and turn them into embeddings. For this we'll import some more necessary packages and select an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97f87731-f51e-4e91-b97e-49826071b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# We'll use this model for the embedding\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Get the model by name, and provide it a device\n",
    "embedding_model = SentenceTransformer(model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d62912-ddb0-48d1-a16c-c63016aa5f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e50cc386b8413381949e2d4f7ff19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = embedding_model.encode(text_chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc294e68-b9de-4862-9dc1-34495f7cd0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "embeddings_size = embeddings[0].shape[0]\n",
    "print(embeddings_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e94d44e-4d5f-45e1-9675-73786817bee9",
   "metadata": {},
   "source": [
    "## Store Embeddings in a Vector Database\n",
    "\n",
    "We'll be using Qdrant for our vector database. See some documentation below:\n",
    "\n",
    "### Python\n",
    "\n",
    "```\n",
    "pip install qdrant-client\n",
    "```\n",
    "\n",
    "The python client offers a convenient way to start with Qdrant locally:\n",
    "\n",
    "```python\n",
    "from qdrant_client import QdrantClient\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance, for testing, CI/CD\n",
    "# OR\n",
    "client = QdrantClient(path=\"path/to/db\")  # Persists changes to disk, fast prototyping\n",
    "```\n",
    "\n",
    "### Client-Server\n",
    "\n",
    "To experience the full power of Qdrant locally, run the container with this command:\n",
    "\n",
    "```bash\n",
    "docker run -p 6333:6333 qdrant/qdrant\n",
    "```\n",
    "\n",
    "Now you can connect to this with any client, including Python:\n",
    "\n",
    "```python\n",
    "qdrant = QdrantClient(\"http://localhost:6333\") # Connect to existing Qdrant instance\n",
    "```\n",
    "\n",
    "### Ok start the docker container in a new terminal session and leave it running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1198911-f064-4f6c-809e-d3c02131a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install qdrant-client\n",
    "\n",
    "# Import client library\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e296f24-0801-4929-95c4-e992605ad456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_model.get_sentence_embedding_dimension()\n",
    "collection_name = \"qa_index\"\n",
    "client.delete_collection(collection_name)\n",
    "\n",
    "# Create our collection and ensure that the vector params size is equal to the size of the embeddings' shape.\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=embeddings_size, distance=Distance.COSINE),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd686b-6f34-493f-850f-fd105190e3ec",
   "metadata": {},
   "source": [
    "## Create payloads and ids (metadata)\n",
    "\n",
    "For each of our vectors/embeddings, we should store metadata that a) identifies the embedding and b) includes metadata about the embedding that is meaningful to us humans (the actual string content of the chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a214d19-5fbb-4eaf-ac84-b8efe68c9ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/Users/michaeldownes/Library/Mobile Documents/iCloud~md~obsidian/Documents/Cloud Vault/Attachments/EXAMPLE-PDF.pdf',\n",
       " 'content': \"Drylab Newsfor in vestors & friends · Ma y 2017\\nWelcome to our first newsletter of 2017! It's\\nbeen a while since the last one, and a lot has\\nhappened. W e promise to k eep them coming\\nevery two months hereafter , and permit\\nourselv es to mak e this one r ather long. The\\nbig news is the beginnings of our launch in\\nthe American mark et, but there are also\\ninteresting updates on sales, de velopment,\\nmentors and ( of course ) the in vestment\\nround that closed in January .\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = []\n",
    "payload = []\n",
    "\n",
    "for id, text in enumerate(text_chunks):\n",
    "    ids.append(id)\n",
    "    payload.append({\"source\": FILE_PATH, \"content\": text})\n",
    "\n",
    "payload[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d870191-ca5d-4d06-98e7-11bda3b04731",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors=embeddings,\n",
    "    payload=payload,\n",
    "    ids=ids,\n",
    "    batch_size=256,  # How many vectors will be uploaded in a single request?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "617dbb27-1ccf-4889-b10f-4bb477c721d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.count(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04dd96-b7a1-402d-9029-ce0cce0f633a",
   "metadata": {},
   "source": [
    "We should have the same number of embeddings here^ as the number of chunks we had from earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc7620-b278-498d-bfe8-738798b99eb5",
   "metadata": {},
   "source": [
    "## Recapping\n",
    "\n",
    "1. We read one PDF File and extract it's text using the `PdfReader` from **pypdf**\n",
    "2. We split the pdf text into chunks with a fixed length\n",
    "3. We embedded the chunks using a model from **sentence_transformers**\n",
    "4. We store those embeddings and their metadata in **Qdrant** vector db\n",
    "\n",
    "### What's left?\n",
    "\n",
    "- Retrieval (querying our local data and getting relevant results)\n",
    "- Response Generation (passing our query and relevant info to an LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035fca9-1337-451d-a910-c3cdf42f7207",
   "metadata": {},
   "source": [
    "## Retrieval component\n",
    "\n",
    "It's time to search our data! Let's define a function that takes a **query** and a **number of chunks** to return from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf6ac610-8bcf-45dd-9464-c6638fb75c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(text: str, top_k: int):\n",
    "    query_embedding = embedding_model.encode(text).tolist() # use the same embedding model to embed our query\n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=None,\n",
    "        limit=top_k\n",
    "    )\n",
    "    return search_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91bc58bf-173a-4de5-8c5e-5fef36f77cab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=4, version=0, score=0.59253424, payload={'source': '/Users/michaeldownes/Library/Mobile Documents/iCloud~md~obsidian/Documents/Cloud Vault/Attachments/EXAMPLE-PDF.pdf', 'content': \"Canada. Lumiere Numeriques ha ve started\\nusing us in F rance. W e also ha ve new\\ncustomers in Norwa y, and high-profile users\\nsuch as Gareth Un win, producer of Oscar-\\nwinning The King's Speech . Re venue for the\\nfirst four months is 200 kNOK, compared to\\n339 kNOK for all of 2016. W e are working\\non a partnership to safeguard sales in\\nNorwa y while beginning to focus more on\\nthe US.\\nNew team members: We've extended our\\norganization with two permanent de velopers\"}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=1, version=0, score=0.559265, payload={'source': '/Users/michaeldownes/Library/Mobile Documents/iCloud~md~obsidian/Documents/Cloud Vault/Attachments/EXAMPLE-PDF.pdf', 'content': 'round that closed in January .\\nNew c apital: The in vestment round was\\nsuccessful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation\\nNorwa y. Including the de velopment\\nagreement with Filmlance International, the\\ntotal new capital is 5 MNOK, partly tied to\\nthe successful completion of milestones. All\\nformalities associated with this process are\\nnow finalized.\\nNew o wners: We would especially lik e to\\nwarmly welcome our new owners to the'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=3, version=0, score=0.53909266, payload={'source': '/Users/michaeldownes/Library/Mobile Documents/iCloud~md~obsidian/Documents/Cloud Vault/Attachments/EXAMPLE-PDF.pdf', 'content': 'Google ·IBM ·Inno vation Norwa y(NY C)·Inno vation Norwa y(SF) ·International\\nCinematogr aphers Guild ·NBC ·Local 871 ·Netflix ·Pomfort ·Radiant Images ·\\nScreening Room · Signiant · Moods of Norwa y· Tapad · Team Downe yHolmsen, T orstein Hansen, and Jostein\\nAanensen. W e look forward to working with\\nyou!\\nSales: Return customer r ate is now 80%,\\npro ving value and willingness to pa y. Film\\nFactory Montreal is our first customer in\\nCanada. Lumiere Numeriques ha ve started'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=9, version=0, score=0.5386317, payload={'source': '/Users/michaeldownes/Library/Mobile Documents/iCloud~md~obsidian/Documents/Cloud Vault/Attachments/EXAMPLE-PDF.pdf', 'content': \"interesting to note that the y regarded the\\nindie mark et as bigger than their own.\\nAndreas was able to secure us an\\ninvitation to the DIT -WIT party , with some of\\nthe world's leading DIT s in attendance. It was\\na great place for informal feedback on Drylab\\nViewer . The pattern was the same as for\\nother users: Initial polite interest turns to\\nreal enthusiasm the moment someone is able\\nto personally try Drylab Viewer! W e also\\nmet with P omfort and Apple about our on-\"}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id=12, version=0, score=0.5349253, payload={'source': '/Users/michaeldownes/Library/Mobile Documents/iCloud~md~obsidian/Documents/Cloud Vault/Attachments/EXAMPLE-PDF.pdf', 'content': \"the International Broadcasters Con vention\\nin Amsterdam in September , and we are\\nworking hard to get solid feedback from pilot\\nusers before then.\\nAnnual Gener al Meeting: Drylab 's A GM\\nwill be held on June 16th at 15:00. An\\ninvitation will be distributed to all owners\\nwell in advance. W e hope to see y ou there!\\nAs y ou c an see it has been a hectic\\nspring that has giv en us a lot o f\\nconfirmation about our product. W e\\nare no w w orking eagerly and hard\\ntowards the US launch with Dr ylab\"}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is our customer return rate?\"\n",
    "results = search(question, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82860515-adaf-47d2-9763-56eb4492075b",
   "metadata": {},
   "source": [
    "# Last bit - Response Generation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3566ef6d-cc33-4a6a-a5e1-06bddbcfe8aa",
   "metadata": {},
   "source": [
    "Let's ask OpenAI to answer our query, by passing it some instructions, our query, and our reference chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4497988a-997f-45f8-a02c-c735674f0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an assistant for question-answering tasks. Answer the question according only to the given context.\n",
    "If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Our results in the last step include relevance scores, versions, payloads, and other stuff\n",
    "# We'll just send the payload content (the human-readable chunk) to the LLM\n",
    "references = [obj.payload[\"content\"] for obj in results]\n",
    "\n",
    "# I think this is just to separate each reference with some new lines to let the LLM know where one chunk starts & ends\n",
    "context = \"\\n\\n\".join(references) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb79a30-f028-4b1f-bcc7-4d1714285e52",
   "metadata": {},
   "source": [
    "Showtime!\n",
    "\n",
    "*A quick note about the **litellm** package*: `litellm` is useful here because it gives us this `completion` method which standardizes the interface/params for calling an LLM across all of the major LLM providers. [Go to litellm.ai](https://www.litellm.ai/)\n",
    "\n",
    "Each LLM provider might have different syntax for how to actually execute a call to their API. `litellm` abstracts that away for us so that we can change the `model` argument from OpenAI's `\"gpt-3.5-turbo\"` to Anthropic's Claude 3.5 Sonnet without having to do anything else\n",
    "\n",
    "Ok let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d751c6ce-fd4c-40b7-b4a9-04f8f9cd68b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "  api_key=OPENAI_API_KEY,\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"content\": system_prompt.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3927f3b6-cd96-4df1-879b-fbd726dc42c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our customer return rate is now 80%.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7894b14-430b-4eeb-bbbf-90a983704536",
   "metadata": {},
   "source": [
    "### Give me references with answers, please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60615c05-c7ea-4bea-970f-551c19a7815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: Our customer return rate is now 80%.\n",
      "\n",
      "\n",
      "REFERENCES:\n",
      "\n",
      "Reference: [1]: Canada. Lumiere Numeriques ha ve started\n",
      "using us in F rance. W e also ha ve new\n",
      "customers in Norwa y, and high-profile users\n",
      "such as Gareth Un win, producer of Oscar-\n",
      "winning The King's Speech . Re venue for the\n",
      "first four months is 200 kNOK, compared to\n",
      "339 kNOK for all of 2016. W e are working\n",
      "on a partnership to safeguard sales in\n",
      "Norwa y while beginning to focus more on\n",
      "the US.\n",
      "New team members: We've extended our\n",
      "organization with two permanent de velopers\n",
      "\n",
      "Reference: [2]: round that closed in January .\n",
      "New c apital: The in vestment round was\n",
      "successful. W e raised 2.13 MNOK to matchthe 2.05 MNOK loan from Inno vation\n",
      "Norwa y. Including the de velopment\n",
      "agreement with Filmlance International, the\n",
      "total new capital is 5 MNOK, partly tied to\n",
      "the successful completion of milestones. All\n",
      "formalities associated with this process are\n",
      "now finalized.\n",
      "New o wners: We would especially lik e to\n",
      "warmly welcome our new owners to the\n",
      "\n",
      "Reference: [3]: Google ·IBM ·Inno vation Norwa y(NY C)·Inno vation Norwa y(SF) ·International\n",
      "Cinematogr aphers Guild ·NBC ·Local 871 ·Netflix ·Pomfort ·Radiant Images ·\n",
      "Screening Room · Signiant · Moods of Norwa y· Tapad · Team Downe yHolmsen, T orstein Hansen, and Jostein\n",
      "Aanensen. W e look forward to working with\n",
      "you!\n",
      "Sales: Return customer r ate is now 80%,\n",
      "pro ving value and willingness to pa y. Film\n",
      "Factory Montreal is our first customer in\n",
      "Canada. Lumiere Numeriques ha ve started\n",
      "\n",
      "Reference: [4]: interesting to note that the y regarded the\n",
      "indie mark et as bigger than their own.\n",
      "Andreas was able to secure us an\n",
      "invitation to the DIT -WIT party , with some of\n",
      "the world's leading DIT s in attendance. It was\n",
      "a great place for informal feedback on Drylab\n",
      "Viewer . The pattern was the same as for\n",
      "other users: Initial polite interest turns to\n",
      "real enthusiasm the moment someone is able\n",
      "to personally try Drylab Viewer! W e also\n",
      "met with P omfort and Apple about our on-\n",
      "\n",
      "Reference: [5]: the International Broadcasters Con vention\n",
      "in Amsterdam in September , and we are\n",
      "working hard to get solid feedback from pilot\n",
      "users before then.\n",
      "Annual Gener al Meeting: Drylab 's A GM\n",
      "will be held on June 16th at 15:00. An\n",
      "invitation will be distributed to all owners\n",
      "well in advance. W e hope to see y ou there!\n",
      "As y ou c an see it has been a hectic\n",
      "spring that has giv en us a lot o f\n",
      "confirmation about our product. W e\n",
      "are no w w orking eagerly and hard\n",
      "towards the US launch with Dr ylab\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"ANSWER: {response.choices[0].message.content}\\n\\n\")\n",
    "print(f\"REFERENCES:\\n\")\n",
    "for index, ref in enumerate(references):\n",
    "    print(f\"Reference: [{index + 1}]: {ref}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c6067-c48a-4da1-8873-51f0872a858e",
   "metadata": {},
   "source": [
    "# Final word\n",
    "\n",
    "Now that we've run through the basic pipeline of a single file, there are a couple ways we can make things more complicated but useful:\n",
    "\n",
    "1. Use multiple PDF files\n",
    "2. Use multiple files of different types (i.e. markdown, txt, and pdfs)\n",
    "3. *Use even more file types (i.e. text, image, audio)*\n",
    "\n",
    "Additionally, there are a few thigns we can do to make level up our developer skills:\n",
    "\n",
    "1. Use a local model running with `Ollama`\n",
    "2. Use `langchain` to abstract away the plumbing of our RAG system\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Let's see if we can repeat this notebook for multiple PDFs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-for-beginners",
   "language": "python",
   "name": "rag-for-beginners"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
