{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48603864-cd4c-4ed9-a44d-59624bd4d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to add this to suppress a warning during the embedding process below. Not part of the tutorial really...\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9044a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52df2f",
   "metadata": {},
   "source": [
    "## First change - Multiple PDFs\n",
    "\n",
    "Leveling up from our basic single-file tutorial, we'll now attempt to handle multiple PDFs.\n",
    "\n",
    "1. Let's create a function for reading PDFs (we know how to do this from our previous notebook)\n",
    "2. Let's create a function to process a folder, retreiving the contents of pdfs in that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092bd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from pypdf import PdfReader\n",
    "\n",
    "# Define a function to extract text from PDF files\n",
    "def read_pdf(file_path: str) -> str:\n",
    "    reader = PdfReader(file_path)\n",
    "    pdf_text = \"\"\n",
    "    for page in reader.pages:\n",
    "        pdf_text += page.extract_text()\n",
    "    return pdf_text\n",
    "\n",
    "# Define a function to process a folder of pdfs\n",
    "# The return is a dictionary where the keys are the file names and the values are the textual content\n",
    "def process_pdf_folder(folder_path: str) -> Dict[str, str]:\n",
    "    print(\"Processing PDFs in folder:\", folder_path)\n",
    "    pdf_contents = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        print(\"Processing file:\", filename)\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            pdf_contents[filename] = read_pdf(file_path) # Use our pdf reader here\n",
    "    return pdf_contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c88069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs in folder: /Users/michaeldownes/Downloads/muitiple-pdfs-rag-demo\n",
      "Processing file: equities.pdf\n",
      "Processing file: labeling.pdf\n",
      "Processing file: pro worker ai policy.pdf\n",
      "Processing file: llms.pdf\n",
      "Processing file: climate change paper.pdf\n",
      "equities.pdf 35461\n",
      "labeling.pdf 38714\n",
      "pro worker ai policy.pdf 32328\n",
      "llms.pdf 19956\n",
      "climate change paper.pdf 22363\n"
     ]
    }
   ],
   "source": [
    "# Set up our folder path\n",
    "FOLDER_PATH = r\"/Users/michaeldownes/Downloads/muitiple-pdfs-rag-demo\"\n",
    "\n",
    "pdf_dictionary = process_pdf_folder(FOLDER_PATH)\n",
    "\n",
    "# Let's take a peek at the dictory\n",
    "# For each key-value pair in the dictionary, print the key and the length of the value\n",
    "for key, value in pdf_dictionary.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd248d",
   "metadata": {},
   "source": [
    "# Time to chunk again (simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542f41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(pdf_dictionary: Dict[str, str]) -> List[Dict[str, str]]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) # hardcoding this for now\n",
    "    chunks = []\n",
    "    for filename, content in pdf_dictionary.items(): # Pass in our dictionary of pdfs to content\n",
    "        print(f\"Chunking {filename}\")\n",
    "        document_chunks = text_splitter.split_text(content) # Split the pdf content\n",
    "        print(f\"Number of chunks: {len(document_chunks)}\")\n",
    "        chunks.extend([{\"source\": filename, \"content\": chunk} for chunk in document_chunks]) # Add the chunks to our list\n",
    "\n",
    "    print(f\"Total number of chunks across all documents: {len(chunks)}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246d8dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking equities.pdf\n",
      "Number of chunks: 79\n",
      "Chunking labeling.pdf\n",
      "Number of chunks: 92\n",
      "Chunking pro worker ai policy.pdf\n",
      "Number of chunks: 73\n",
      "Chunking llms.pdf\n",
      "Number of chunks: 45\n",
      "Chunking climate change paper.pdf\n",
      "Number of chunks: 50\n",
      "Total number of chunks across all documents: 339\n",
      "equities.pdf 471\n",
      "equities.pdf 467\n",
      "equities.pdf 488\n"
     ]
    }
   ],
   "source": [
    "# lets try a chunking process to view some print statements\n",
    "chunks = chunk_documents(pdf_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b00ae4",
   "metadata": {},
   "source": [
    "# Embed and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c9aed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def embed_chunks(chunks: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\" # Hardcoding a model for now\n",
    "    embedding_model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    texts = [chunk[\"content\"] for chunk in chunks]\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978dc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "def store_in_db(chunks: List[Dict[str, str]], embeddings: torch.Tensor):\n",
    "    client = QdrantClient(\"http://localhost:6333\")\n",
    "    collection_name=\"qa_index\" # using the same index from the basic example on purpose here\n",
    "\n",
    "    # Delete collection if it already exists\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embeddings.shape[1], distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "    # Prepare payload and IDs\n",
    "    payload = [{\"content\": chunk[\"content\"], \"source\": chunk[\"source\"]} for chunk in chunks]\n",
    "    ids = list(range(len(chunks)))\n",
    "\n",
    "    # Upload to Qdrant\n",
    "    client.upload_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors=embeddings,\n",
    "        payload=payload,\n",
    "        ids=ids,\n",
    "        batch_size=256\n",
    "    )\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4449a59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c292f28518944cbad342ae9c86cbb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddsings of shape: (339, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_chunks(chunks)\n",
    "print(f\"Created embeddsings of shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d33d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored count=339 vectors in the collection\n"
     ]
    }
   ],
   "source": [
    "client = store_in_db(chunks, embeddings)\n",
    "print(f\"Stored {client.count(collection_name='qa_index')} vectors in the collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d0b3e",
   "metadata": {},
   "source": [
    "# Recap\n",
    "- Folder files read\n",
    "- Folder files added to dictionary (filename: content)\n",
    "- Chunk list created (list of objects with keys `source` and `content`)\n",
    "- Embeddings created (vectorize each chunk.content)\n",
    "- Store embeddings with payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38bfbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(text: str, top_k: int):\n",
    "    # Because of scope differences, I copied the embedding_modle code here from the embed_chunks function\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\" # Hardcoding a model for now\n",
    "    embedding_model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    # Embed the query text\n",
    "    query_embedding = embedding_model.encode(text).tolist() \n",
    "\n",
    "    search_result = client.search(\n",
    "        collection_name='qa_index',\n",
    "        query_vector=query_embedding,\n",
    "        query_filter=None,\n",
    "        limit=top_k\n",
    "    )\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "511ef0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: AstraZeneca installed three highly efficient heat pumps at their site in Gothenburg, Sweden, which has allowed them to drastically reduce the use of gas and electrify some of the site's steam demand. This initiative has the potential to replace over 60% of the site's gas consumption, contributing to greenhouse gas reduction efforts.\n",
      "\n",
      "\n",
      "REFERENCES:\n",
      "\n",
      "Reference: [1]: projects and continuing to work with key supply \n",
      "chain partners to help them achieve greenhouse gas \n",
      "reductions.ASTRAZENECA\n",
      "As part of AstraZeneca’s GHG reduction program \n",
      "(validated for the first time by SBTi in 2016), Astra \n",
      "Zeneca’s site in Gothenburg, Sweden has installed \n",
      "three highly efficient heat pumps, which has \n",
      "allowed it to drastically reduce the use of gas. It \n",
      "electrifies some of the site’s steam demand and \n",
      "has the potential to replace over 60% of site gas \n",
      "consumption.\n",
      "\n",
      "Reference: [2]: o AbbVie 2018 Responsible Action Report  and AbbVie - Prioritizing environmental sustainability\n",
      "o AstraZeneca’s Ambition Zero Carbon strategy  and Environmental Protection webpage  \n",
      "o Bayer Climate Protection webpage  \n",
      "o Biogen – Position Paper  2020\n",
      "o Chiesi - SUSTAINABILITY REPORT 2019\n",
      "o GSK – Climate Change Position Paper\n",
      "o IPSEN- Financial report 2019 (p 165 – 167)  \n",
      "o Johnson & Johnson (Janssen) – Climate & Energy Commitments\n",
      "o Lundbeck – UN Global Compact Progress Report\n",
      "\n",
      "Reference: [3]: The driving motivation of the pharmaceutical industry is to improve human health and wellbeing. It has been well \n",
      "documented that climate change can adversely impact human health. Further understanding of these impacts \n",
      "and the interface between people, health and the environment is critical to ensuring the pharmaceutical industry \n",
      "can form and execute our response.\n",
      "   \n",
      "This White Paper highlights the commitment made by the EFPIA companies to:\n",
      "\n",
      "Reference: [4]: White Paper on Climate Change\n",
      "2The pharmaceutical industry is committed to making a positive impact on the lives of patients while operating \n",
      "sustainably and therefore strives to contribute to a healthy environment and demonstrate leadership in doing \n",
      "what’s necessary to mitigate climate change. This commitment is aligned with the ambition the European \n",
      "Commission recently expressed through their European Green Deal and the European Climate policies.\n",
      "\n",
      "Reference: [5]: o Lundbeck – UN Global Compact Progress Report\n",
      "o Merck – Climate Action Report 201 9\n",
      "o MSD – Environmental sustainability 2020\n",
      "o Novartis – Climate commitments\n",
      "o Novo Nordisk Climate Position on Climate Change  \n",
      "o Pfizer – Climate Change Position\n",
      "o Roche – Position on Climate Change April 2020\n",
      "o Sanofi – April 2019\n",
      "o Takeda Commitment to Carbon Neutralit y and Position statement on climate change\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "# Retrieve\n",
    "\n",
    "# Let's start with an easier question\n",
    "question = \"What has Astrazeneca done to reduce the effects of climate change?\"\n",
    "results = search(question, top_k=5)\n",
    "\n",
    "\n",
    "# Generate\n",
    "system_prompt = \"\"\"You are an assistant for question-answering tasks. Answer the question according only to the given context.\n",
    "If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Our results in the last step include relevance scores, versions, payloads, and other stuff\n",
    "# We'll just send the payload content (the human-readable chunk) to the LLM\n",
    "references = [obj.payload[\"content\"] for obj in results]\n",
    "\n",
    "# I think this is just to separate each reference with some new lines to let the LLM know where one chunk starts & ends\n",
    "context = \"\\n\\n\".join(references) \n",
    "\n",
    "response = completion(\n",
    "  api_key=OPENAI_API_KEY,\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[{\"content\": system_prompt.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}]\n",
    ")\n",
    "\n",
    "print(f\"ANSWER: {response.choices[0].message.content}\\n\\n\")\n",
    "print(f\"REFERENCES:\\n\")\n",
    "for index, ref in enumerate(references):\n",
    "    print(f\"Reference: [{index + 1}]: {ref}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd1fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ragOpenAI(question: str):\n",
    "    results = search(question, top_k=5)\n",
    "    references = [obj.payload[\"content\"] for obj in results]\n",
    "    \n",
    "    system_prompt = \"\"\"You are an assistant for question-answering tasks. Answer the question according only to the given context.\n",
    "    If question cannot be answered using the context, simply say I don't know. Do not make stuff up.\n",
    "\n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = \"\"\"\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    # Our results in the last step include relevance scores, versions, payloads, and other stuff\n",
    "    # We'll just send the payload content (the human-readable chunk) to the LLM\n",
    "    references = [obj.payload[\"content\"] for obj in results]\n",
    "\n",
    "    # I think this is just to separate each reference with some new lines to let the LLM know where one chunk starts & ends\n",
    "    context = \"\\n\\n\".join(references) \n",
    "\n",
    "    response = completion(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\", messages=[{\"content\": system_prompt.format(context=context),\"role\": \"system\"}, {\"content\": user_prompt.format(question=question),\"role\": \"user\"}])\n",
    "    \n",
    "    print(f\"ANSWER: {response.choices[0].message.content}\\n\\n\")\n",
    "    print(f\"REFERENCES:\\n\")\n",
    "    for index, ref in enumerate(references):\n",
    "        print(f\"Reference: [{index + 1}]: {ref}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b29d752c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: There are at least two models for how regulation of LLMs might be carried out. One model involves a progressive tightening of standards, allowing \"unsafe\" models to be deployed with notifications to users, while the other model focuses on disclosing when LLMs are used in regulated domains and requiring the storage of model weights and parameters for investigations. Additionally, regulation should require risk assessment by model providers before release and could be done through existing domain-specific regulations.\n",
      "\n",
      "\n",
      "REFERENCES:\n",
      "\n",
      "Reference: [1]: with relatively minimal retraining of the model. Regulation should require developers to assess potential risks prior to deployment and establish liability for developers that distribute models that are used to cause foreseeable harm.  Part 3: Innovations that Could Improve Large Language Models  LLM technology is still in a state of flux. There are a number of potential technical innovations that could help mitigate the safety concerns articulated above but that do not yet exist or are\n",
      "\n",
      "Reference: [2]: capabilities). These are, therefore, research topics on which regulation and increased public investment might have an outsized impact.  Part 4:  Modes of Regulation  There are at least two models, not mutually exclusive, for how regulation of LLMs might be carried out. One model involves a progressive tightening of standards: a regulatory regime might permit “unsafe” (or insufficiently safe) models to be deployed today, while requiring providers of such models to notify users of potential\n",
      "\n",
      "Reference: [3]: when LLMs are being used, regulation might require that companies disclose when language models are used in a regulated domain and clarify that systems need to comply with existing rules and regulations. To support investigations into model uses, regulation should require that copies of the weights and parameters of models used in regulated domains be stored for a set period of time.  Hosted models. In the case of hosted models like ChatGPT that facilitate use by customers (either through\n",
      "\n",
      "Reference: [4]: a model is being used at all. On the other hand, the deploying entity is in a strong position to understand and decide how the model is used. Because development and deployment are performed by a single entity, it is most natural to regulate these models through existing domain-specific regulations.   For example, an in-house model used for resume filtering would need to comply with existing non-discrimination laws. Because it may be hard to determine when LLMs are being used, regulation might\n",
      "\n",
      "Reference: [5]: service, or as downloadable models. Regardless of the method of release, regulation should require risk assessment by the model provider before release.     • Potential technical innovations that would improve LLM safety include verifiable attribution, hard-to-remove watermarks, guaranteed forgetting, better guardrails, and auditability. These can help mitigate safety concerns, improve user trust, and prevent misuse. Regulation and funding should be structured to encourage work on, and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try a more difficult question\n",
    "question=\"How might we regulate LLMs?\"\n",
    "response = ragOpenAI(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b53b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
